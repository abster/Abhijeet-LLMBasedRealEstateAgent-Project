{
 "cells": [
  {
   "cell_type": "code",
   "id": "a39747ab-f962-46a9-aa0e-150e21628c9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T23:51:29.419857Z",
     "start_time": "2025-03-28T23:51:27.135908Z"
    }
   },
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "import csv\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_experimental.open_clip.open_clip import OpenCLIPEmbeddings\n",
    "\n",
    "from langchain_chroma.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import LanceDB\n",
    "import chromadb\n",
    "from langchain.chains import RetrievalQA\n",
    "from IPython.display import Markdown, display\n",
    "import re\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<Your-OpenAI-Key>\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://openai.vocareum.com/v1\"\n",
    "\n",
    "regenerate_listings = False\n",
    "multi_modal_mode = True\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "example_listing = \"\"\"\n",
    "Neighborhood: Green Oaks\n",
    "Price: $800,000\n",
    "Bedrooms: 3\n",
    "Bathrooms: 2\n",
    "House Size: 2,000 sqft\n",
    "\n",
    "Description: Welcome to this eco-friendly oasis nestled in the heart of Green Oaks. This charming 3-bedroom, 2-bathroom home boasts energy-efficient features such as solar panels and a well-insulated structure. Natural light floods the living spaces, highlighting the beautiful hardwood floors and eco-conscious finishes. The open-concept kitchen and dining area lead to a spacious backyard with a vegetable garden, perfect for the eco-conscious family. Embrace sustainable living without compromising on style in this Green Oaks gem.\n",
    "\n",
    "Neighborhood Description: Green Oaks is a close-knit, environmentally-conscious community with access to organic grocery stores, community gardens, and bike paths. Take a stroll through the nearby Green Oaks Park or grab a cup of coffee at the cozy Green Bean Cafe. With easy access to public transportation and bike lanes, commuting is a breeze.\n",
    "\"\"\"\n",
    "\n",
    "listing_field_names = ['Listing Number', 'Neighborhood', 'Price', 'Bedrooms', 'Bathrooms', 'House Size','Description', 'Neighborhood Description']\n",
    "\n",
    "base_prompt = (\"Generate a realistic and detailed real estate listing. You can refer to the provided example for how to \"\n",
    "               \"structure the real estate listing:\\n {example}\")\n",
    "prompt = PromptTemplate.from_template(\n",
    "    base_prompt + \"\\n\\n Do not repeat already generated listings: {generated_listings}\")\n",
    "summarization_prompt = PromptTemplate.from_template(\n",
    "    \"Summarize briefly in less than 10 words, capturing essential details of the specified real estate listing: {listing}\")\n",
    "\n",
    "generation_chain = prompt | llm\n",
    "summarization_chain = summarization_prompt | llm\n",
    "\n",
    "listings = []\n",
    "summaries = []\n",
    "\n",
    "generate_listings = not os.path.exists(\"generated-real-estate-listings.csv\") or regenerate_listings\n",
    "\n",
    "if generate_listings:\n",
    "    print(\"Generating real estate listings...\")\n",
    "\n",
    "    # Generate 10 real-estate listings using LLM.\n",
    "    # We generate 1 listing at a time to account for LLM's context window, and pass summaries of already generated listings\n",
    "    # to prevent generating repeated/duplicated listings.\n",
    "    for i in range(10):\n",
    "        listing = generation_chain.invoke({\"example\": example_listing, \"generated_listings\": summaries})\n",
    "        listings.append(listing.content)\n",
    "        summary = summarization_chain.invoke({\"listing\": listing.content})\n",
    "        summaries.append(summary.content)\n",
    "\n",
    "    print(f\"Generated {len(listings)} real estate listings!\\n\\n\")\n",
    "\n",
    "    # Save listings as csv file\n",
    "    with open('generated-real-estate-listings.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=\"|\", quoting=csv.QUOTE_STRINGS)\n",
    "        writer.writerow(listing_field_names)\n",
    "        for index, listing in enumerate(listings):\n",
    "            updated_listing = re.sub(r\"\\n+\", \"|\", listing)\n",
    "            listing_fields = updated_listing.split(\"|\")\n",
    "            updated_listing_fields = [ listing_field.replace(f\"{listing_field_name}: \", \"\")\n",
    "                                       for listing_field_name, listing_field in zip(listing_field_names[1:], listing_fields) ]\n",
    "            writer.writerow([ index + 1 ] + updated_listing_fields)\n",
    "        print(f\"The generated real estate listings were written to generated-real-estate-listings.csv.\\n\\n\")\n",
    "\n",
    "# Load real estate listings from CSV file in a Vector database.\n",
    "loader = CSVLoader(file_path='generated-real-estate-listings.csv', csv_args={ 'delimiter': '|',\n",
    "                                                                              'fieldnames': listing_field_names })\n",
    "print(\"Loading real estate listings...\")\n",
    "listings_data = loader.load()\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "if not os.path.exists(\"./chroma\"):\n",
    "    os.makedirs(\"./chroma\")\n",
    "    os.chmod(\"./chroma\", 0o777) # permissions 0o777\n",
    "\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "db = Chroma(\n",
    "    client=persistent_client,\n",
    "    collection_name=\"real-estate-listings-table\",\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "db.add_documents(documents=listings_data, ids = [str(id) for id in range(11)])\n",
    "\n",
    "print(\"Real estate listings added to vector database.\")\n",
    "\n",
    "if multi_modal_mode:\n",
    "    print(\"Loading real estate listing images...\")\n",
    "\n",
    "    # Load real estate images in a Vector database.\n",
    "    clip_embeddings = OpenCLIPEmbeddings()\n",
    "    db_images = LanceDB(\n",
    "        table_name=\"real_estate_images\",\n",
    "        embedding=clip_embeddings,\n",
    "    )\n",
    "    image_uris = [ f\"real-estate-image{row + 1}.jpg\" for row in range(10) ]\n",
    "    db_images.add_images(uris=image_uris, ids = [id for id in range(10)])\n",
    "    print(\"Real estate listing images added to vector database.\")\n",
    "\n",
    "questions = [\n",
    "    \"How big do you want your house to be?\"\n",
    "    \"What are the 3 most important things for you in choosing this property?\",\n",
    "    \"Which amenities would you like?\",\n",
    "    \"Which transportation options are important to you?\",\n",
    "    \"How urban do you want your neighborhood to be?\",\n",
    "]\n",
    "answers = [\n",
    "    \"A luxurious 4-bedroom house with a spacious kitchen and a cozy living room.\",\n",
    "    \"An upscale neighborhood, top rated schools, and nearby shopping.\",\n",
    "    \"An outdoor pool, patio, and spa like bathroom.\",\n",
    "    \"Easy access to parks, walking trails and club houses\",\n",
    "    \"A balance between suburban tranquility and access to urban amenities like restaurants and theaters.\",\n",
    "    ]\n",
    "\n",
    "visual_questions = [\n",
    "    \"What exterior color would you like your house to have?\"\n",
    "]\n",
    "visual_answers = [\n",
    "    \"Tuscan yellow will be cool.\"\n",
    "]\n",
    "\n",
    "user_preferences = [{\"criteria\": question, \"user_response\": answer} for question, answer in zip(questions, answers)]\n",
    "user_visual_preferences = [{\"criteria\": question, \"user_response\": answer} for question, answer in zip(visual_questions, visual_answers)]\n",
    "\n",
    "summarization_prompt = PromptTemplate.from_template(\n",
    "    \"Summarize briefly, capturing essential details of user preferences based on provided questions and answers: {user_preferences}\")\n",
    "summarization_chain = summarization_prompt | llm\n",
    "\n",
    "print(\"Generating a summary of user preferences...\")\n",
    "\n",
    "user_preference_summary = summarization_chain.invoke({\"user_preferences\": user_preferences})\n",
    "user_visual_preference_summary = summarization_chain.invoke({\"user_preferences\": user_visual_preferences})\n",
    "\n",
    "print(f\"Summary of user preferences:\\n{user_preference_summary.content}\\n\")\n",
    "print(f\"Summary of visual preferences:\\n{user_visual_preference_summary.content}\\n\")\n",
    "\n",
    "non_visual_search_results = db.as_retriever().invoke(user_preference_summary.content)\n",
    "matching_listings = [ int(result.id) for result in non_visual_search_results ]\n",
    "\n",
    "print(f\"Listing ids matching user preference (non-visual): {matching_listings}\")\n",
    "\n",
    "if multi_modal_mode:\n",
    "    print(\"Using vector database for images to select top listings based on visual preferences from user...\")\n",
    "    visual_search_results = db_images.similarity_search_by_vector(db_images._embedding.embed_query(user_visual_preference_summary.content))\n",
    "    visual_matches = [ int(result.metadata[\"id\"]) + 1 for result in visual_search_results ]\n",
    "    print(f\"Listing ids matching user preference (visual): {visual_matches}\")\n",
    "    intersection = list(set(matching_listings) & set(visual_matches))\n",
    "    print(f\"Listing ids matching both user preference (non-visual) and user preference(visual): {intersection}\")\n",
    "    if len(intersection) == 0:\n",
    "        print(f\"Sticking with listing ids matching user preference (non-visual): {matching_listings}\")\n",
    "    else:\n",
    "        matching_listings = intersection\n",
    "\n",
    "description_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\",\n",
    "      \"You are a real-estate agent. Enhance the specified real estate listing description, so that it is tailored towards \"\n",
    "      \"the user preference. Ensure that the description only includes factual information that was present in the specified \"\n",
    "      \"real estate listing. You do not need to start with the phrase \\\"Enhanced real estate listing\\\".\"),\n",
    "     (\"human\", \"Real estate listing:\\n{description}\\n User preference:\\n{user_preference}\")]\n",
    ")\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "for index, matching_listing in enumerate(matching_listings):\n",
    "    row = int(matching_listing)\n",
    "    print(f\"Enhancing description for the listing {matching_listing} to match user preferences...\")\n",
    "    recommendation = llm.invoke(description_prompt.format(description=listings_data[row].page_content,\n",
    "                                  user_preference=user_preference_summary.content))\n",
    "    recommendations.append(recommendation.content)\n",
    "\n",
    "display({'text/plain': \"## Recommendations from LLM\\n\",\n",
    "         'text/markdown': \"## Recommendations from LLM\\n\"},\n",
    "        raw=True)\n",
    "for matching_listing, recommendation in zip(matching_listings, recommendations):\n",
    "    row = int(matching_listing)\n",
    "    display({'text/plain': f\"### Listing {row}\\n\",\n",
    "             'text/markdown': f\"### Listing {row}\\n\"},\n",
    "            raw=True)\n",
    "    image_uri = f\"./real-estate-image{row}.jpg\"\n",
    "    display(Markdown(f\"![Picture for listing]({image_uri})\"))\n",
    "    display({'text/plain': recommendation,\n",
    "             'text/markdown': recommendation},\n",
    "            raw=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading real estate listings...\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'code': None, 'message': 'Insufficient budget available', 'param': None, 'type': 'invalid_request_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mBadRequestError\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 102\u001B[39m\n\u001B[32m     95\u001B[39m persistent_client = chromadb.PersistentClient()\n\u001B[32m     96\u001B[39m db = Chroma(\n\u001B[32m     97\u001B[39m     client=persistent_client,\n\u001B[32m     98\u001B[39m     collection_name=\u001B[33m\"\u001B[39m\u001B[33mreal-estate-listings-table\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     99\u001B[39m     embedding_function=embeddings,\n\u001B[32m    100\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m102\u001B[39m \u001B[43mdb\u001B[49m\u001B[43m.\u001B[49m\u001B[43madd_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocuments\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlistings_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mids\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mid\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mid\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[32;43m11\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    104\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mReal estate listings added to vector database.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    106\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m multi_modal_mode:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RealEstateAgent/.venv/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:287\u001B[39m, in \u001B[36mVectorStore.add_documents\u001B[39m\u001B[34m(self, documents, **kwargs)\u001B[39m\n\u001B[32m    285\u001B[39m     texts = [doc.page_content \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m documents]\n\u001B[32m    286\u001B[39m     metadatas = [doc.metadata \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m documents]\n\u001B[32m--> \u001B[39m\u001B[32m287\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43madd_texts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadatas\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    288\u001B[39m msg = (\n\u001B[32m    289\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m`add_documents` and `add_texts` has not been implemented \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    290\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mfor \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    291\u001B[39m )\n\u001B[32m    292\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(msg)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RealEstateAgent/.venv/lib/python3.12/site-packages/langchain_chroma/vectorstores.py:532\u001B[39m, in \u001B[36mChroma.add_texts\u001B[39m\u001B[34m(self, texts, metadatas, ids, **kwargs)\u001B[39m\n\u001B[32m    530\u001B[39m texts = \u001B[38;5;28mlist\u001B[39m(texts)\n\u001B[32m    531\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._embedding_function \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m532\u001B[39m     embeddings = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_embedding_function\u001B[49m\u001B[43m.\u001B[49m\u001B[43membed_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    533\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m metadatas:\n\u001B[32m    534\u001B[39m     \u001B[38;5;66;03m# fill metadatas with empty dicts if somebody\u001B[39;00m\n\u001B[32m    535\u001B[39m     \u001B[38;5;66;03m# did not specify metadata for all texts\u001B[39;00m\n\u001B[32m    536\u001B[39m     length_diff = \u001B[38;5;28mlen\u001B[39m(texts) - \u001B[38;5;28mlen\u001B[39m(metadatas)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RealEstateAgent/.venv/lib/python3.12/site-packages/langchain_openai/embeddings/base.py:588\u001B[39m, in \u001B[36mOpenAIEmbeddings.embed_documents\u001B[39m\u001B[34m(self, texts, chunk_size)\u001B[39m\n\u001B[32m    585\u001B[39m \u001B[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001B[39;00m\n\u001B[32m    586\u001B[39m \u001B[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001B[39;00m\n\u001B[32m    587\u001B[39m engine = cast(\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mself\u001B[39m.deployment)\n\u001B[32m--> \u001B[39m\u001B[32m588\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_len_safe_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[43m=\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RealEstateAgent/.venv/lib/python3.12/site-packages/langchain_openai/embeddings/base.py:483\u001B[39m, in \u001B[36mOpenAIEmbeddings._get_len_safe_embeddings\u001B[39m\u001B[34m(self, texts, engine, chunk_size)\u001B[39m\n\u001B[32m    481\u001B[39m batched_embeddings: List[List[\u001B[38;5;28mfloat\u001B[39m]] = []\n\u001B[32m    482\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m _iter:\n\u001B[32m--> \u001B[39m\u001B[32m483\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    484\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m=\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43m_chunk_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_invocation_params\u001B[49m\n\u001B[32m    485\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    486\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mdict\u001B[39m):\n\u001B[32m    487\u001B[39m         response = response.model_dump()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RealEstateAgent/.venv/lib/python3.12/site-packages/openai/resources/embeddings.py:128\u001B[39m, in \u001B[36mEmbeddings.create\u001B[39m\u001B[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m    122\u001B[39m             embedding.embedding = np.frombuffer(  \u001B[38;5;66;03m# type: ignore[no-untyped-call]\u001B[39;00m\n\u001B[32m    123\u001B[39m                 base64.b64decode(data), dtype=\u001B[33m\"\u001B[39m\u001B[33mfloat32\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    124\u001B[39m             ).tolist()\n\u001B[32m    126\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\u001B[32m--> \u001B[39m\u001B[32m128\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    129\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m/embeddings\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    130\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mEmbeddingCreateParams\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    131\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    132\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    133\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    134\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    135\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    136\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpost_parser\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    137\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    138\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mCreateEmbeddingResponse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    139\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RealEstateAgent/.venv/lib/python3.12/site-packages/openai/_base_client.py:1242\u001B[39m, in \u001B[36mSyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[39m\n\u001B[32m   1228\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1229\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1230\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1237\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1238\u001B[39m ) -> ResponseT | _StreamT:\n\u001B[32m   1239\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1240\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001B[32m   1241\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1242\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RealEstateAgent/.venv/lib/python3.12/site-packages/openai/_base_client.py:919\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[39m\n\u001B[32m    916\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    917\u001B[39m     retries_taken = \u001B[32m0\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m919\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    920\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    921\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    922\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    923\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    924\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    925\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RealEstateAgent/.venv/lib/python3.12/site-packages/openai/_base_client.py:1023\u001B[39m, in \u001B[36mSyncAPIClient._request\u001B[39m\u001B[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[39m\n\u001B[32m   1020\u001B[39m         err.response.read()\n\u001B[32m   1022\u001B[39m     log.debug(\u001B[33m\"\u001B[39m\u001B[33mRe-raising status error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1023\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._make_status_error_from_response(err.response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1025\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._process_response(\n\u001B[32m   1026\u001B[39m     cast_to=cast_to,\n\u001B[32m   1027\u001B[39m     options=options,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1031\u001B[39m     retries_taken=retries_taken,\n\u001B[32m   1032\u001B[39m )\n",
      "\u001B[31mBadRequestError\u001B[39m: Error code: 400 - {'error': {'code': None, 'message': 'Insufficient budget available', 'param': None, 'type': 'invalid_request_error'}}"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "f932a670-f07b-44d5-bdd7-aed83211b5bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T22:46:10.020333Z",
     "start_time": "2025-03-28T22:46:10.013646Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a6c9b0-623e-4951-a7b2-b500a31d590d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
